{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d44528-17a8-4c55-a544-594ea1a5455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b3c579c6-ab39-407c-ae78-f858aed2dde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import json\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def load_stopwords(file_path='./stopwords.txt'):\n",
    "    \"\"\"\n",
    "    从文件中加载停用词，并存储在集合中\n",
    "    :param file_path: 停用词文件的路径，默认为 'chinese_stopwords.txt'\n",
    "    :return: 停用词集合\n",
    "    \"\"\"\n",
    "    stopwords = set()\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            stopwords.add(line.strip())\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "def load_user_dict(user_dict_path='./user_dict.txt'):\n",
    "    \"\"\"\n",
    "    加载用户自定义词典，只添加词语本身\n",
    "    :param user_dict_path: 用户自定义词典的路径，默认为 'user_dict.txt'\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    with open(user_dict_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word = line.strip()\n",
    "            jieba.add_word(word)\n",
    "\n",
    "\n",
    "def tokenize_text(text, stopwords):\n",
    "    \"\"\"\n",
    "    使用 jieba 对文本进行分词，并去除停用词和单字（只保留汉字）\n",
    "    :param text: 输入的文本\n",
    "    :param stopwords: 停用词集合\n",
    "    :return: 去除停用词和单字后的分词列表\n",
    "    \"\"\"\n",
    "    tokens = jieba.cut(text)\n",
    "    # 过滤掉只包含空格或特殊字符的词以及单字\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if len(token) > 1 and all('\\u4e00' <= char <= '\\u9fff' for char in token) and token.strip() and not all(c in ' \\n\\t\\r:.,!?[](){}\"\\'`' for c in token):\n",
    "            if token not in stopwords:\n",
    "                filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "def process_video(video_info, stopwords):\n",
    "    \"\"\"\n",
    "    处理单个视频的评论和弹幕\n",
    "    :param video_info: 包含视频评论和弹幕的字典\n",
    "    :param stopwords: 停用词集合\n",
    "    :return: 处理后的视频信息字典\n",
    "    \"\"\"\n",
    "    # 处理评论\n",
    "    processed_comments = []\n",
    "    for comment in video_info[\"所有评论\"]:\n",
    "        processed_comments.extend(tokenize_text(comment, stopwords))\n",
    "    video_info[\"所有评论\"] = processed_comments\n",
    "\n",
    "    # 处理弹幕\n",
    "    processed_bullets = []\n",
    "    for bullet in video_info[\"所有弹幕\"]:\n",
    "        processed_bullets.extend(tokenize_text(bullet, stopwords))\n",
    "    video_info[\"所有弹幕\"] = processed_bullets\n",
    "    return video_info\n",
    "\n",
    "\n",
    "def process_partition(partition):\n",
    "    \"\"\"\n",
    "    处理分区中的所有视频\n",
    "    :param partition: 分区信息的字典，包含多个视频的信息\n",
    "    :return: 处理后的分区信息字典\n",
    "    \"\"\"\n",
    "    stopwords = load_stopwords()\n",
    "    processed_partition = {}\n",
    "    for video_id, video_info in partition.items():\n",
    "        processed_partition[video_id] = process_video(video_info, stopwords)\n",
    "    return processed_partition\n",
    "\n",
    "\n",
    "def analyze_word_frequency(video_info):\n",
    "    \"\"\"\n",
    "    对视频的评论和弹幕进行词频分析，统计前 20 的词频\n",
    "    :param video_info: 视频信息的字典\n",
    "    :return: 词频统计结果\n",
    "    \"\"\"\n",
    "    word_counts = Counter()\n",
    "    for comment in video_info[\"所有评论\"]:\n",
    "        word_counts.update(tokenize_text(comment, load_stopwords()))\n",
    "    for bullet in video_info[\"所有弹幕\"]:\n",
    "        word_counts.update(tokenize_text(bullet, load_stopwords()))\n",
    "    return word_counts.most_common(20)\n",
    "\n",
    "\n",
    "def process_json(input_json_path, output_json_path, output_csv_path):\n",
    "    \"\"\"\n",
    "    处理整个 JSON 文件并生成 CSV 文件\n",
    "    :param input_json_path: 输入 JSON 文件的路径\n",
    "    :param output_json_path: 输出 JSON 文件的路径\n",
    "    :param output_csv_path: 输出 CSV 文件的路径\n",
    "    \"\"\"\n",
    "    with open(input_json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_data = {}\n",
    "    with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['分区', '视频bv', '高频词']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for partition_name, partition in data.items():\n",
    "            processed_partition = process_partition(partition)\n",
    "            processed_data[partition_name] = processed_partition\n",
    "            for video_id, video_info in processed_partition.items():\n",
    "                top_20_word_counts = analyze_word_frequency(video_info)\n",
    "                top_20_words = [word for word, count in top_20_word_counts]\n",
    "                writer.writerow({'分区': partition_name, '视频bv': video_id, '高频词': top_20_words})\n",
    "\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(processed_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "def main():\n",
    "    input_json_path = './video_info.json' \n",
    "    output_json_path = './output.json' \n",
    "    output_csv_path = 'word_frequency.csv'  \n",
    "    process_json(input_json_path, output_json_path, output_csv_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb0122d-455f-4101-bccb-8a7a72ce01c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "178baf97-fe49-4040-bc57-2c5250ce3b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud\n",
    "import json\n",
    "\n",
    "\n",
    "def generate_word_cloud(partition_name, text, font_path):\n",
    "    \"\"\"\n",
    "    生成词云图\n",
    "    :param partition_name: 分区名称\n",
    "    :param text: 要生成词云的文本\n",
    "    :param font_path: 本地字体文件的路径\n",
    "    \"\"\"\n",
    "    # 创建词云对象，使用本地字体，并设置浅色背景\n",
    "    wordcloud_obj = wordcloud.WordCloud(\n",
    "        width=800,\n",
    "        height=600,\n",
    "        font_path=font_path,\n",
    "        background_color='white'  # 设置背景颜色为白色，你可以根据需要修改为其他浅色，例如 'lightgray'\n",
    "    ).generate(text)\n",
    "\n",
    "    # 保存词云图为文件\n",
    "    output_file = f\"./worldcloud/{partition_name}_word_cloud.png\"\n",
    "    wordcloud_obj.to_file(output_file)\n",
    "\n",
    "\n",
    "def process_json(json_file_path, font_path):\n",
    "    \"\"\"\n",
    "    处理 JSON 文件并生成词云图\n",
    "    :param json_file_path: JSON 文件的路径\n",
    "    :param font_path: 本地字体文件的路径\n",
    "    \"\"\"\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for partition_name, partition in data.items():\n",
    "        all_text = \"\"\n",
    "        for video in partition.values():\n",
    "            all_text += \" \".join(video[\"所有评论\"])\n",
    "            all_text += \" \".join(video[\"所有弹幕\"])\n",
    "\n",
    "        # 生成词云图\n",
    "        generate_word_cloud(partition_name, all_text, font_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    json_file_path = './output.json'  \n",
    "    font_path = './YeZiGongChangTangYingHei-2.ttf'  \n",
    "    process_json(json_file_path, font_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d8830f6a-baf4-4ff5-840b-af319433384d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据已保存至 ./output_withdanmaku_csv.csv\n"
     ]
    }
   ],
   "source": [
    "#这个是带弹幕的版本\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "def json_to_csv(json_file_path, csv_file_path):\n",
    "    # 读取 JSON 文件\n",
    "    with open(json_file_path, 'r', encoding='utf-8-sig') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    rows = []\n",
    "    for partition, videos in data.items():\n",
    "        for video_id, video_info in videos.items():\n",
    "            # 合并所有评论和弹幕\n",
    "            all_text = \" \".join(video_info.get(\"所有评论\", [])) + \" \" + \" \".join(video_info.get(\"所有弹幕\", []))\n",
    "            row = {\n",
    "                \"分区\": partition,\n",
    "                \"视频 ID\": video_id,\n",
    "                \"所有评论和弹幕\": all_text\n",
    "            }\n",
    "            rows.append(row)\n",
    "    \n",
    "    # 将数据转换为 DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # 添加自增序列作为序号\n",
    "    df.insert(0, \"序号\", range(1, len(df) + 1))\n",
    "    \n",
    "    # 保存为 CSV 文件\n",
    "    df.to_csv(csv_file_path, index=False)\n",
    "    print(f\"数据已保存至 {csv_file_path}\")\n",
    "\n",
    "\n",
    "# 调用函数\n",
    "json_file_path = './output.json'  \n",
    "csv_file_path = './output_withdanmaku_csv.csv' \n",
    "json_to_csv(json_file_path, csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc71151-11e0-4d26-a41b-9363550bfc92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
